{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Concatenate, Add, GRU\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing add features function from Feature engineering file\n",
    "import ipynb\n",
    "from ipynb.fs.full.Feature_Engineering import add_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"C:\\\\Users\\\\Ajay Kallepalli\\\\Documents\\\\GitHub\\\\Kaggle Data\\\\test.csv\")\n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\Ajay Kallepalli\\\\Documents\\\\GitHub\\\\Kaggle Data\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11d721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df258d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data...\\n\")\n",
    "train = add_features(train_data)\n",
    "print(\"\\nTest data...\\n\")\n",
    "test = add_features(test_data)\n",
    "del train_data\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
    "\n",
    "train.drop(['pressure','id', 'breath_id','one','count',\n",
    "            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n",
    "            'breath_id_lag2same'], axis=1, inplace=True)\n",
    "\n",
    "test = test.drop(['id', 'breath_id','one','count','breath_id_lag',\n",
    "                  'breath_id_lag2','breath_id_lagsame',\n",
    "                  'breath_id_lag2same'], axis=1)\n",
    "\n",
    "print(f\"train: {train.shape} \\ntest: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ff8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "test = test.reshape(-1, 80, train.shape[-1])\n",
    "\n",
    "print(f\"train: {train.shape} \\ntest: {test.shape} \\ntargets: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842822d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure = targets.squeeze().reshape(-1,1).astype('float32')\n",
    "\n",
    "P_MIN = np.min(pressure)\n",
    "P_MAX = np.max(pressure)\n",
    "P_STEP = (pressure[1] - pressure[0])[0]\n",
    "print('Min pressure: {}'.format(P_MIN))\n",
    "print('Max pressure: {}'.format(P_MAX))\n",
    "print('Pressure step: {}'.format(P_STEP))\n",
    "print('Unique values:  {}'.format(np.unique(pressure).shape[0]))\n",
    "\n",
    "del pressure\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f5bdb",
   "metadata": {},
   "source": [
    "### Configurating Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fabb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 512\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35350a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model():\n",
    "    \n",
    "    x_input = Input(shape=(train.shape[-2:]))\n",
    "    \n",
    "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
    "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
    "    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n",
    "    \n",
    "    z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n",
    "    z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n",
    "    \n",
    "    x = Concatenate(axis=2)([x3, z2, z3])\n",
    "    x = Bidirectional(LSTM(units=192, return_sequences=True))(x)\n",
    "    \n",
    "    x = Dense(units=128, activation='selu')(x)\n",
    "    \n",
    "    x_output = Dense(units=1)(x)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output, \n",
    "                  name='DNN_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89421c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    VERBOSE = 0\n",
    "    test_preds = []\n",
    "    kf = KFold(n_splits=7, shuffle=True, random_state=2021)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "        X_train, X_valid = train[train_idx], train[test_idx]\n",
    "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
    "        \n",
    "        model = dnn_model()\n",
    "        model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n",
    "                               patience=10, verbose=VERBOSE)\n",
    "        \n",
    "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "        chk_point = ModelCheckpoint(f'./Bidirect_LSTM_model_{fold+1}C.h5', options=save_locally, \n",
    "                                    monitor='val_loss', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min')\n",
    "\n",
    "        es = EarlyStopping(monitor=\"val_loss\", patience=50, \n",
    "                           verbose=VERBOSE, mode=\"min\", \n",
    "                           restore_best_weights=True)\n",
    "        \n",
    "        model.fit(X_train, y_train, \n",
    "                  validation_data=(X_valid, y_valid), \n",
    "                  epochs=300,\n",
    "                  verbose=VERBOSE,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  callbacks=[lr, chk_point, es])\n",
    "        \n",
    "        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "        model = load_model(f'./Bidirect_LSTM_model_{fold+1}C.h5', options=load_locally)\n",
    "        \n",
    "        y_true = y_valid.squeeze().reshape(-1, 1)\n",
    "        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n",
    "        score = mean_absolute_error(y_true, y_pred)\n",
    "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
    "        \n",
    "        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d215d",
   "metadata": {},
   "source": [
    "### Creating submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n",
    "submission[\"pressure\"] = sum(test_preds)/7\n",
    "submission.to_csv('mean_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
    "submission[\"pressure\"] = np.round((submission.pressure - P_MIN)/P_STEP) * P_STEP + P_MIN\n",
    "submission[\"pressure\"] = np.clip(submission.pressure, P_MIN, P_MAX)\n",
    "submission.to_csv('median_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
